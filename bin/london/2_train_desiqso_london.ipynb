{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d049d93c-8f97-491c-9b93-70a206fa9167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from accelerate import Accelerator\n",
    "from spender import SpectrumAutoencoder\n",
    "from spender.data import desi_qso as desi \n",
    "from spender.util import mem_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172c7ccb-0d22-41d8-84a4-60b6fb753d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_train(seq,niter=800):\n",
    "    for d in seq:\n",
    "        if not \"iteration\" in d:d[\"iteration\"]=niter\n",
    "        if not \"encoder\" in d:d.update({\"encoder\":d[\"data\"]})\n",
    "    return seq\n",
    "\n",
    "def build_ladder(train_sequence):\n",
    "    n_iter = sum([item['iteration'] for item in train_sequence])\n",
    "\n",
    "    ladder = np.zeros(n_iter,dtype='int')\n",
    "    n_start = 0\n",
    "    for i,mode in enumerate(train_sequence):\n",
    "        n_end = n_start+mode['iteration']\n",
    "        ladder[n_start:n_end]= i\n",
    "        n_start = n_end\n",
    "    return ladder\n",
    "\n",
    "def get_all_parameters(models,instruments):\n",
    "    model_params = []\n",
    "    # multiple encoders\n",
    "    for model in models:\n",
    "        model_params += model.encoder.parameters()\n",
    "        \n",
    "    print(sum([p.numel() for p in model_params if p.requires_grad]))\n",
    "    # 1 decoder\n",
    "    model_params += model.decoder.parameters()\n",
    "    dicts = [{'params':model_params}]\n",
    "\n",
    "    n_parameters = sum([p.numel() for p in model_params if p.requires_grad])\n",
    "\n",
    "    instr_params = []\n",
    "    # instruments\n",
    "    for inst in instruments:\n",
    "        if inst==None:continue\n",
    "        instr_params += inst.parameters()\n",
    "        s = [p.numel() for p in inst.parameters()]\n",
    "    if instr_params != []:\n",
    "        dicts.append({'params':instr_params,'lr': 1e-4})\n",
    "        n_parameters += sum([p.numel() for p in instr_params if p.requires_grad])\n",
    "        print(\"parameter dict:\",dicts[1])\n",
    "    return dicts,n_parameters\n",
    "\n",
    "def restframe_weight(model,mu=5000,sigma=2000,amp=30):\n",
    "    x = model.decoder.wave_rest\n",
    "    return amp*torch.exp(-(0.5*(x-mu)/sigma)**2)\n",
    "\n",
    "def Loss(model, instrument, batch):\n",
    "    spec, w, z = batch\n",
    "    # need the latents later on if similarity=True\n",
    "    s = model.encode(spec)\n",
    "    \n",
    "    return model.loss(spec, w, instrument, z=z, s=s)\n",
    "\n",
    "def checkpoint(accelerator, args, optimizer, scheduler, n_encoder, outfile, losses):\n",
    "    unwrapped = [accelerator.unwrap_model(args_i).state_dict() for args_i in args]\n",
    "\n",
    "    accelerator.save({\n",
    "        \"model\": unwrapped,\n",
    "        \"losses\": losses,\n",
    "    }, outfile)\n",
    "    return\n",
    "\n",
    "def load_model(filename, models, instruments):\n",
    "    device = instruments[0].wave_obs.device\n",
    "    model_struct = torch.load(filename, map_location=device)\n",
    "    #wave_rest = model_struct['model'][0]['decoder.wave_rest']\n",
    "    for i, model in enumerate(models):\n",
    "        # backwards compat: encoder.mlp instead of encoder.mlp.mlp\n",
    "        if 'encoder.mlp.mlp.0.weight' in model_struct['model'][i].keys():\n",
    "            from collections import OrderedDict\n",
    "            model_struct['model'][i] = OrderedDict([(k.replace('mlp.mlp', 'mlp'), v) for k, v in model_struct['model'][i].items()])\n",
    "        # backwards compat: add instrument to encoder\n",
    "        try:\n",
    "            model.load_state_dict(model_struct['model'][i], strict=False)\n",
    "        except RuntimeError:\n",
    "            model_struct['model'][i]['encoder.instrument.wave_obs']= instruments[i].wave_obs\n",
    "            model_struct['model'][i]['encoder.instrument.skyline_mask']= instruments[i].skyline_mask\n",
    "            model.load_state_dict(model_struct[i]['model'], strict=False)\n",
    "\n",
    "    losses = model_struct['losses']\n",
    "    return models, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01671e9c-b53a-4cc9-b958-a988ebc83188",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restframe:\t1161 .. 9824 A (9780 bins)\n",
      "/tigress/chhahn/spender_qso/train\n"
     ]
    }
   ],
   "source": [
    "z_max = 2.1\n",
    "_dir = '/tigress/chhahn/spender_qso/train'\n",
    "outfile = '/tigress/chhahn/spender_qso/train/models/testing.pt'\n",
    "latents = 10 \n",
    "lr = 1e-3\n",
    "\n",
    "# define instruments\n",
    "instruments = [ desi.DESI() ]\n",
    "n_encoder = len(instruments)\n",
    "\n",
    "# data loaders\n",
    "batch_size = 256\n",
    "trainloaders = [ inst.get_data_loader(_dir, tag=\"london_highz\", which=\"train\",  batch_size=batch_size, shuffle=True, shuffle_instance=True) for inst in instruments ]\n",
    "validloaders = [ inst.get_data_loader(_dir,  tag=\"london_highz\", which=\"valid\", batch_size=batch_size, shuffle=True, shuffle_instance=True) for inst in instruments ]\n",
    "\n",
    "# restframe wavelength for reconstructed spectra\n",
    "# Note: represents joint dataset wavelength range\n",
    "lmbda_min = instruments[0].wave_obs[0]/(1.0+z_max) # 2000 A\n",
    "lmbda_max = instruments[0].wave_obs[-1] # 9824 A\n",
    "bins = 9780\n",
    "wave_rest = torch.linspace(lmbda_min, lmbda_max, bins, dtype=torch.float32)\n",
    "    \n",
    "print (\"Restframe:\\t{:.0f} .. {:.0f} A ({} bins)\".format(lmbda_min, lmbda_max, bins))\n",
    "\n",
    "print(_dir) \n",
    "\n",
    "\n",
    "# define training sequence\n",
    "FULL = {\"data\":[True],\"decoder\":True}\n",
    "train_sequence = prepare_train([FULL])\n",
    "\n",
    "# define and train the model\n",
    "n_hidden = (64, 128, 1024)\n",
    "models = [ SpectrumAutoencoder(instrument,\n",
    "                               wave_rest,\n",
    "                               n_latent=latents,\n",
    "                               n_hidden=n_hidden,\n",
    "                               act=[nn.LeakyReLU()]*(len(n_hidden)+1)\n",
    "                               )\n",
    "          for instrument in instruments ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "930ee253-7975-4fbc-a16c-b9b88c564621",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.device_count(): 1\n",
      "--- Model /tigress/chhahn/spender_qso/train/models/testing.pt ---\n"
     ]
    }
   ],
   "source": [
    "n_epoch = sum([item['iteration'] for item in train_sequence])\n",
    "init_t = time.time()\n",
    "print(\"torch.cuda.device_count():\",torch.cuda.device_count())\n",
    "print (f\"--- Model {outfile} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1165a2b9-bc5d-4cfd-bce1-f4865ed74cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3159178\n",
      "model parameters: 13324798\n",
      "CPU RAM Free: 1.1 TB\n",
      "GPU 0 ... Mem Free: 81342MB / 81920MB | Utilization   0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "n_encoder = len(models)\n",
    "model_parameters, n_parameters = get_all_parameters(models,instruments)\n",
    "\n",
    "print(\"model parameters:\", n_parameters)\n",
    "mem_report()\n",
    "\n",
    "ladder = build_ladder(train_sequence)\n",
    "optimizer = torch.optim.Adam(model_parameters, lr=lr, eps=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, lr,\n",
    "                                          total_steps=n_epoch)\n",
    "\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "models = [accelerator.prepare(model) for model in models]\n",
    "instruments = [accelerator.prepare(instrument) for instrument in instruments]\n",
    "trainloaders = [accelerator.prepare(loader) for loader in trainloaders]\n",
    "validloaders = [accelerator.prepare(loader) for loader in validloaders]\n",
    "optimizer = accelerator.prepare(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b61d2e7-516c-4a27-bcab-448e05313c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU RAM Free: 1.1 TB\n",
      "GPU 0 ... Mem Free: 80858MB / 81920MB | Utilization   1%\n",
      "====> Epoch: 0\n",
      "TRAINING Losses: (8.909975073832555,)\n",
      "VALIDATION Losses: (3.9447492397085364,)\n",
      "CPU RAM Free: 1.1 TB\n",
      "GPU 0 ... Mem Free: 61784MB / 81920MB | Utilization  24%\n",
      "====> Epoch: 1\n",
      "TRAINING Losses: (3.6204224470965296,)\n",
      "VALIDATION Losses: (3.0830632733197776,)\n",
      "CPU RAM Free: 1.1 TB\n",
      "GPU 0 ... Mem Free: 61784MB / 81920MB | Utilization  24%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m n_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(validloaders[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 44\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruments\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# logging: validation\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     detailed_loss[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,epoch_] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss \u001b[38;5;66;03m#tuple( l.item() if hasattr(l, 'item') else 0 for l in losses )\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [2], line 52\u001b[0m, in \u001b[0;36mLoss\u001b[0;34m(model, instrument, batch)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# need the latents later on if similarity=True\u001b[39;00m\n\u001b[1;32m     50\u001b[0m s \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(spec)\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstrument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/spender_qso/spender/model.py:545\u001b[0m, in \u001b[0;36mBaseAutoencoder.loss\u001b[0;34m(self, y, w, instrument, z, s, normalize, individual)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, w, instrument\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, z\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, individual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;124;03m\"\"\"Weighted MSE loss\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m    Parameter\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03m    float or `torch.tensor`, shape (N,) of weighted MSE loss\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m     y_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstrument\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstrument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(y, w, y_, individual\u001b[38;5;241m=\u001b[39mindividual)\n",
      "File \u001b[0;32m~/.conda/envs/gqp/lib/python3.8/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gqp/lib/python3.8/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/gqp/lib/python3.8/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/spender_qso/spender/model.py:515\u001b[0m, in \u001b[0;36mBaseAutoencoder.forward\u001b[0;34m(self, y, instrument, z, s, normalize, weights)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, instrument\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, z\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;124;03m\"\"\"Forward method\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m    Transforms observed spectra into their reconstruction for a given intrument and redshift. If weights are passed, also normalizes the reconstruction to the observed spectrum.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m        Batch of spectra at redshift `z` as observed by `instrument`\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m     s, x, y_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstrument\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstrument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_\n",
      "File \u001b[0;32m~/projects/spender_qso/spender/model.py:477\u001b[0m, in \u001b[0;36mBaseAutoencoder._forward\u001b[0;34m(self, y, instrument, z, s, normalize, weights)\u001b[0m\n\u001b[1;32m    475\u001b[0m restframe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# make resampled and interpolated reconstruction\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m reconstruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrestframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstrument\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstrument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# normalize restframe and reconstruction to observed spectrum\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n",
      "File \u001b[0;32m~/projects/spender_qso/spender/model.py:338\u001b[0m, in \u001b[0;36mSpectrumDecoder.transform\u001b[0;34m(self, x, instrument, z)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    337\u001b[0m     wave_obs \u001b[38;5;241m=\u001b[39m instrument\u001b[38;5;241m.\u001b[39mwave_obs\n\u001b[0;32m--> 338\u001b[0m spectrum \u001b[38;5;241m=\u001b[39m \u001b[43minterp1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwave_redshifted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwave_obs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# convolve with LSF\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instrument \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m instrument\u001b[38;5;241m.\u001b[39mlsf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# track training and validation loss\n",
    "detailed_loss = np.zeros((2, n_encoder, n_epoch))\n",
    "\n",
    "for epoch_ in range(n_epoch):\n",
    "    mem_report()\n",
    "    mode = train_sequence[ladder[epoch_]]\n",
    "\n",
    "    # turn on/off model decoder\n",
    "    for p in models[0].decoder.parameters():\n",
    "        p.requires_grad = True #mode['decoder']\n",
    "\n",
    "    # turn on/off encoder\n",
    "    for p in models[0].encoder.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    models[0].train()\n",
    "    instruments[0].train()\n",
    "\n",
    "    n_sample = 0\n",
    "    for k, batch in enumerate(trainloaders[0]):\n",
    "        loss = Loss(models[0], instruments[0], batch)\n",
    "        \n",
    "        accelerator.backward(loss)\n",
    "        # clip gradients: stabilizes training with similarity\n",
    "        accelerator.clip_grad_norm_(model_parameters[0]['params'], 1.0)\n",
    "        # once per batch\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # logging: training\n",
    "        detailed_loss[0,0,epoch_] += loss #tuple( l.item() if hasattr(l, 'item') else 0 for l in losses )\n",
    "        n_sample += batch_size\n",
    "\n",
    "    detailed_loss[0,0,epoch_] /= n_sample\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        models[0].eval()\n",
    "        instruments[0].eval()\n",
    "\n",
    "        n_sample = 0\n",
    "        for k, batch in enumerate(validloaders[0]):\n",
    "            loss = Loss(models[0], instruments[0], batch)\n",
    "            # logging: validation\n",
    "            detailed_loss[1,0,epoch_] += loss #tuple( l.item() if hasattr(l, 'item') else 0 for l in losses )\n",
    "            n_sample += batch_size\n",
    "\n",
    "        detailed_loss[1,0,epoch_] /= n_sample\n",
    "\n",
    "    losses = tuple(detailed_loss[0, :, epoch_])\n",
    "    vlosses = tuple(detailed_loss[1, :, epoch_])\n",
    "    print('====> Epoch: %i' % (epoch_))\n",
    "    print('TRAINING Losses:', losses)\n",
    "    print('VALIDATION Losses:', vlosses)\n",
    "\n",
    "    #if epoch_ % 5 == 0 or epoch_ == n_epoch - 1:\n",
    "    #    args = models\n",
    "    #    checkpoint(accelerator, args, optimizer, scheduler, n_encoder, outfile, detailed_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195b5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab6f299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gqp [~/.conda/envs/gqp/]",
   "language": "python",
   "name": "conda_gqp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
