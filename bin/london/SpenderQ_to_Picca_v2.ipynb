{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01a970d-b1ad-4270-aaaa-88a4ee322c38",
   "metadata": {},
   "source": [
    "# Matching SpenderQ output to Picca \"delta_extraction\" format\n",
    "\n",
    "This notebook is a copy of the SpenderQ_to_Picca.ipynb. Changes include:\n",
    "- creating a loop over ibatch for the entire notebook. This means the initial spenderq arrays are reset during each iteration.\n",
    "- when saving the delta files they are saved as delta-ibatch instead of delta-ichunk, since with the new loop there is only one ichunk per ibatch (given the chunk size of 1024 remains unchanged). \n",
    "\n",
    "Future (potential) updates include changes to save the delta files under their healpix number instead of ibatch number, but it remains to be seen if this is a problem with picca yet. \n",
    "\n",
    "This notebook takes SpenderQ outputs, and reformats it to match the \"delta files\" format obtained by Picca via \"delta_extraction\"; making it compatible with picca to calculate the correlation functions. [2025 February 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8489b5-4f65-4f92-bf8d-36ba73f6be61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b329597-b972-4ea8-9100-86369e2b2162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de10023-54aa-4bc7-9d30-2085cd488b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spenderq import load_model\n",
    "from spenderq import util as U\n",
    "from spenderq import lyalpha as LyA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6456dbf-b919-4f33-8035-48f0cdf74b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ff1f68-8e97-46f2-8da1-b0d9e25b9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import healpy as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f35172-94b5-4c5b-abd3-e825e53db2df",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Add paths, define variables, load catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27553d1-0adf-4602-9c25-7729895646a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quasar catalogue of the production of interest (fits file)\n",
    "\n",
    "## EDR\n",
    "#quasar_catalogue = '/global/cfs/projectdirs/desi/users/sgontcho/lya/spender-lya/QSO_cat_EDR_n_M2_main_dark_healpix_BAL_n_DLA_cuts.fits'\n",
    "\n",
    "## Y1 LONDON MOCKS\n",
    "# quasar_catalogue = '/global/cfs/cdirs/desicollab/mocks/lya_forest/develop/london/qq_desi/v9.0_Y1/v9.0.9.9.9/desi-4.124-4-prod/zcat.fits'\n",
    "quasar_catalogue = '/global/cfs/cdirs/desi/users/abault/spenderq/zcat_hpx.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b41e0f9b-60f9-478a-aab9-124ae2219a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE THESE FUNCTIONS WILL ONLY RUN IF healpy was installed and imported above\n",
    "#we created a separate catalog that added the healpix information, path is above\n",
    "\n",
    "def calculate_healpix(catalog, nside):\n",
    "    #calculates healpix values given a catalog and nside value\n",
    "    hpx = hp.ang2pix(nside, np.pi/2-np.deg2rad(catalog['DEC']), np.deg2rad(catalog['RA']))\n",
    "    catalog['HPXPIXEL'] = hpx\n",
    "\n",
    "def add_healpix(catalog, nside = 8):\n",
    "    #if healpix is not already in catalog, calculate and add a column\n",
    "    #desi data catalogs use 'HPXPIXEL' as column name\n",
    "    #default nside value in picca_cf is 16, this produced 1032 healpixs which I thought was too many, so I went with nside=8 (302)\n",
    "    \n",
    "    if 'HPXPIXEL' in catalog.columns:\n",
    "        return catalog\n",
    "    else:\n",
    "        #add healpix to catalog\n",
    "        catalog = calculate_healpix(catalog, nside)\n",
    "        return catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0952679c-3aca-478a-9a81-dc24649d867c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#point to the SpenderQ output directory and give the files prefix (from the same production as the catalogue!)\n",
    "\n",
    "path = '/global/cfs/projectdirs/desi/users/chahah/spender_qso'\n",
    "#outpath = '/global/cfs/projectdirs/desi/users/sgontcho/lya/spender-lya/spenderq-to-deltas'\n",
    "# outpath = '/global/cfs/projectdirs/desi/users/sgontcho/lya/spender-lya/iron_comparison/spenderq_prod_20241126_v0/Delta'\n",
    "outpath = '/global/cfs/projectdirs/desi/users/abault/spenderq/deltas_lya/test_hpx'\n",
    "\n",
    "## Y1 LONDON MOCKS\n",
    "file_prefix = 'spenderq_london_v0/DESIlondon_highz.rebin.iter3'\n",
    "spender_output_files = glob.glob(path+'/'+file_prefix+'_*.pkl')\n",
    "\n",
    "## EDR\n",
    "#file_prefix = 'DESI.edr.qso_highz'\n",
    "#spender_output_files = glob.glob('/global/cfs/projectdirs/desi/users/chahah/spender_qso/DESIedr.qso_highz_*.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05e90c9e-baee-42de-b8fd-d4e6084d950e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LAMBDA range defined in picca\n",
    "picca_lambda_min = 3600.\n",
    "picca_lambda_max = 5772.\n",
    "\n",
    "#FOREST range (default is LyA)\n",
    "forest_lambda_min = 1040.\n",
    "forest_lambda_max = 1205."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbf2acef-6086-47e1-8730-f65383b0fe92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdul = fits.open(quasar_catalogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ae7c974-194d-47f2-938c-9e0253129aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "catalogue_TID = hdul[1].data['TARGETID']\n",
    "catalogue_Z = hdul[1].data['Z']\n",
    "catalogue_RA = hdul[1].data['RA']\n",
    "catalogue_DEC = hdul[1].data['DEC']\n",
    "catalogue_HPX = hdul[1].data['HPXPIXEL']\n",
    "\n",
    "##EDR\n",
    "#catalogue_RA = hdul[1].data['TARGET_RA']\n",
    "#catalogue_DEC = hdul[1].data['TARGET_DEC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1370b60a-cf9b-4d03-b3ef-d9abeab1d4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LAMBDA = np.arange(picca_lambda_min,picca_lambda_max+0.8,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ebf044d-d1c3-45ef-9532-cce007aec6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab all values from spenderq and merge into one array\n",
    "normalised_flux, pipeline_weight, z, tid, normalisation, zerr = [], [], [], [], [], []\n",
    "\n",
    "sq_reconstructions = []\n",
    "\n",
    "for ibatch in range(50):\n",
    "    # print(ibatch)\n",
    "#for ibatch in range(len(spender_output_files)): \n",
    "    \n",
    "    #load batch\n",
    "    with open(f'{path}/{file_prefix}_%i.pkl' % ibatch, 'rb') as f:\n",
    "        _normalised_flux, _pipeline_weight, _z, _tid, _normalisation, _zerr = pickle.load(f)\n",
    "    normalised_flux.append(np.array(_normalised_flux))\n",
    "    pipeline_weight.append(np.array(_pipeline_weight))\n",
    "    z.append(np.array(_z))\n",
    "    tid.append(np.array(_tid))\n",
    "    normalisation.append(np.array(_normalisation))\n",
    "    zerr.append(np.array(_zerr))\n",
    "    \n",
    "    #load SpenderQ reconstructions\n",
    "    _sq_reconstructions = np.load(f'{path}/{file_prefix}_%i.recons.npy' % ibatch)\n",
    "    sq_reconstructions.append(np.array(_sq_reconstructions))\n",
    "\n",
    "normalised_flux=np.concatenate(normalised_flux,axis=0)\n",
    "pipeline_weight=np.concatenate(pipeline_weight,axis=0)\n",
    "z=np.concatenate(z)\n",
    "tid=np.concatenate(tid)\n",
    "normalisation=np.concatenate(normalisation,axis=0)\n",
    "zerr=np.concatenate(zerr,axis=0)\n",
    "sq_reconstructions=np.concatenate(sq_reconstructions,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb56f24-cb6a-423b-b892-4e7157ec711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb_of_quasars = 10\n",
    "nb_of_quasars = len(z)\n",
    "\n",
    "_tff = np.full((nb_of_quasars,7781),np.nan)\n",
    "_weight = np.full((nb_of_quasars,7781),np.nan)\n",
    "_sq_cont = np.full((nb_of_quasars,7781),np.nan)\n",
    "\n",
    "wrecon = np.load(f'{path}/{file_prefix}.wave_recon.npy')\n",
    "desi_wave = np.linspace(3600, 9824, 7781) #obs\n",
    "\n",
    "_tff_bar_up = np.zeros(7781)\n",
    "_tff_bar_down = np.zeros(7781)\n",
    "\n",
    "#for iqso in range(10):\n",
    "for iqso in range(nb_of_quasars):\n",
    "\n",
    "    #create wavelength grids\n",
    "    desi_wave_rest = desi_wave/(1+z[iqso]) #rest\n",
    "    \n",
    "    #rebin spender reconstruction\n",
    "    edges = np.linspace(wrecon[0], wrecon[-1], 7782)\n",
    "    spenderq_rebin = U.trapz_rebin(wrecon, sq_reconstructions[iqso], edges = edges)\n",
    "    \n",
    "    #keep only part of spec that is within the lya range\n",
    "    mask_desi_rest = (desi_wave_rest >= forest_lambda_min) & (desi_wave_rest <= forest_lambda_max)\n",
    "\n",
    "    _tff[iqso][mask_desi_rest] = normalised_flux[iqso][mask_desi_rest]/spenderq_rebin[mask_desi_rest]\n",
    "    _weight[iqso][mask_desi_rest] = pipeline_weight[iqso][mask_desi_rest]\n",
    "    _sq_cont[iqso][mask_desi_rest] = spenderq_rebin[mask_desi_rest]\n",
    "    _tff_bar_up += normalised_flux[iqso]/spenderq_rebin * pipeline_weight[iqso]\n",
    "    _tff_bar_down += pipeline_weight[iqso]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e7cae21-a66d-413a-be39-8032da575566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the weighted average transmitted flux fraction \n",
    "picca_range = (desi_wave <= LAMBDA[-1])\n",
    "tff_bar = np.divide(_tff_bar_up[picca_range], _tff_bar_down[picca_range], out=np.zeros_like(_tff_bar_up[picca_range]), where=_tff_bar_down[picca_range]!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3787857-acf3-4675-ab3e-141f3a5433a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#picca structure: nan filled arrays\n",
    "delta = np.full((nb_of_quasars,sum(picca_range)),np.nan)\n",
    "weight = np.full((nb_of_quasars,sum(picca_range)),np.nan)\n",
    "sq_cont = np.full((nb_of_quasars,sum(picca_range)),np.nan)\n",
    "\n",
    "meta_los_id, meta_ra, meta_dec, meta_z, meta_meansnr, meta_targetid, meta_night, meta_petal, meta_tile = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "#for iqso in range(5):\n",
    "for iqso in range(nb_of_quasars):\n",
    "    \n",
    "    delta[iqso] = (_tff[iqso][picca_range]/tff_bar) - 1.\n",
    "    weight[iqso] = _weight[iqso][picca_range]\n",
    "    sq_cont[iqso] = _sq_cont[iqso][picca_range]\n",
    "    \n",
    "    los_id_idx = int(np.where(catalogue_TID == int(tid[iqso]))[0][0])\n",
    "    \n",
    "    meta_los_id.append(int(catalogue_TID[los_id_idx]))\n",
    "    meta_ra.append(float(math.radians(catalogue_RA[los_id_idx])))\n",
    "    meta_dec.append(float(math.radians(catalogue_DEC[los_id_idx])))\n",
    "    meta_z.append(float(catalogue_Z[los_id_idx]))\n",
    "    meta_meansnr.append(np.nan)\n",
    "    meta_targetid.append(int(catalogue_TID[los_id_idx]))\n",
    "    meta_night.append('')\n",
    "    meta_petal.append('')\n",
    "    meta_tile.append('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0238e0bd-d005-4ae7-b51c-de0bb3b1b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create primary HDU \n",
    "primary_hdu = fits.PrimaryHDU(data=None)\n",
    "\n",
    "# Create OBSERVED WAVELENGTH HDU\n",
    "hdu_wave = fits.ImageHDU(LAMBDA, name=f'LAMBDA')\n",
    "hdu_wave.header['HIERARCH WAVE_SOLUTION'] = 'lin'\n",
    "hdu_wave.header['HIERARCH DELTA_LAMBDA'] = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49758bcf-074f-4faa-a9f6-f98c7aaea33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over healpix to get which qsos to save in which delta file:\n",
    "unique_hpx = np.unique(catalogue_HPX)\n",
    "\n",
    "for hpx in unique_hpx:\n",
    "    hpx_mask = catalogue_HPX == hpx\n",
    "    tids_hpx = catalogue_TID[hpx_mask]\n",
    "\n",
    "    spender_mask = np.isin(tid, tids_hpx)\n",
    "    \n",
    "    if np.count_nonzero(spender_mask) != 0:\n",
    "        c1 = fits.Column(name='LOS_ID', format='K', array=np.array(meta_los_id)[spender_mask])\n",
    "        c2 = fits.Column(name='RA', format='D', array=np.array(meta_ra)[spender_mask])\n",
    "        c3 = fits.Column(name='DEC', format='D', array=np.array(meta_dec)[spender_mask])\n",
    "        c4 = fits.Column(name='Z', format='D', array=np.array(meta_z)[spender_mask])\n",
    "        c5 = fits.Column(name='MEANSNR', format='D', array=np.array(meta_meansnr)[spender_mask])\n",
    "        c6 = fits.Column(name='TARGETID', format='K', array=np.array(meta_targetid)[spender_mask])\n",
    "        c7 = fits.Column(name='NIGHT', format='12A', array=np.array(meta_night)[spender_mask])\n",
    "        c8 = fits.Column(name='PETAL', format='12A', array=np.array(meta_petal)[spender_mask])\n",
    "        c9 = fits.Column(name='TILE', format='12A', array=np.array(meta_tile)[spender_mask])\n",
    "        hdu_meta = fits.BinTableHDU.from_columns([c1, c2, c3, c4, c5, c6, c7, c8, c9], name='METADATA')\n",
    "        hdu_meta.header['BLINDING'] = 'none'\n",
    "    \n",
    "        hdu_delta = fits.ImageHDU(delta[spender_mask], name=f'DELTA')\n",
    "        hdu_weight = fits.ImageHDU(weight[spender_mask], name=f'WEIGHT')\n",
    "        hdu_cont = fits.ImageHDU(sq_cont[spender_mask], name=f'CONT')\n",
    "        hdu_tff_bar = fits.ImageHDU(tff_bar, name=f'FBAR')\n",
    "    \n",
    "        hdul1 = fits.HDUList([primary_hdu, hdu_wave, hdu_meta, hdu_delta, hdu_weight, hdu_cont, hdu_tff_bar])\n",
    "    \n",
    "        # Write the HDUList to a new FITS file\n",
    "        hdul1.writeto(f'{outpath}/delta-{hpx}.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e538de-1c83-470e-bbcb-51e6b0032004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f030bfb-857f-4f4b-a316-e90006ab373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create primary HDU \n",
    "primary_hdu = fits.PrimaryHDU(data=None)\n",
    "\n",
    "# Create OBSERVED WAVELENGTH HDU\n",
    "hdu_wave = fits.ImageHDU(LAMBDA, name=f'LAMBDA')\n",
    "hdu_wave.header['HIERARCH WAVE_SOLUTION'] = 'lin'\n",
    "hdu_wave.header['HIERARCH DELTA_LAMBDA'] = 0.8\n",
    "\n",
    "# Set chunk size, and how to chunk\n",
    "chunk_size = 1024\n",
    "\n",
    "def chunks(lst, n):\n",
    "    return [lst[i:i + n] for i in range(0, len(lst), n)]\n",
    "\n",
    "_delta_chunks = chunks(delta, chunk_size)\n",
    "_weight_chunks = chunks(weight, chunk_size)\n",
    "_cont_chunks = chunks(sq_cont, chunk_size)\n",
    "\n",
    "nb_of_chunks = len(_delta_chunks)\n",
    "print(nb_of_chunks)\n",
    "\n",
    "for ichunk in range(nb_of_chunks):\n",
    "    i=0\n",
    "    c1 = fits.Column(name='LOS_ID', format='K', array=np.array(meta_los_id[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c2 = fits.Column(name='RA', format='D', array=np.array(meta_ra[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c3 = fits.Column(name='DEC', format='D', array=np.array(meta_dec[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c4 = fits.Column(name='Z', format='D', array=np.array(meta_z[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c5 = fits.Column(name='MEANSNR', format='D', array=meta_meansnr[i*chunk_size:(i+1)*chunk_size])\n",
    "    c6 = fits.Column(name='TARGETID', format='K', array=np.array(meta_targetid[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c7 = fits.Column(name='NIGHT', format='12A', array=meta_night[i*chunk_size:(i+1)*chunk_size])\n",
    "    c8 = fits.Column(name='PETAL', format='12A', array=meta_petal[i*chunk_size:(i+1)*chunk_size])\n",
    "    c9 = fits.Column(name='TILE', format='12A', array=meta_tile[i*chunk_size:(i+1)*chunk_size])\n",
    "    hdu_meta = fits.BinTableHDU.from_columns([c1, c2, c3, c4, c5, c6, c7, c8, c9], name='METADATA')\n",
    "    hdu_meta.header['BLINDING'] = 'none'\n",
    "\n",
    "    hdu_delta = fits.ImageHDU(delta[spender_mask], name=f'DELTA')\n",
    "    hdu_weight = fits.ImageHDU(weight_chunks[ichunk], name=f'WEIGHT')\n",
    "    hdu_cont = fits.ImageHDU(_cont_chunks[ichunk], name=f'CONT')\n",
    "    hdu_tff_bar = fits.ImageHDU(tff_bar, name=f'FBAR')\n",
    "\n",
    "    # Combine all HDUs into an HDUList\n",
    "    hdul = fits.HDUList([primary_hdu, hdu_wave, hdu_meta, hdu_delta, hdu_weight, hdu_cont, hdu_tff_bar])\n",
    "\n",
    "    # Write the HDUList to a new FITS file\n",
    "    hdul.writeto(f'{outpath}/delta-%i.fits' % ichunk, overwrite=True)\n",
    "    i+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f326e-3c56-4918-a383-e9c5b6e4b80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cd4d8fe-4de6-4ad5-b171-59949c73a7ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# code with bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71ab0ca-37a9-481d-8ce1-c29c2795ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    return [lst[i:i + n] for i in range(0, len(lst), n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7d6fbcc-9ae7-424e-88cb-0373e21e56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spender_to_deltas(ibatchrange = 50):\n",
    "\n",
    "    #loop over ibatch values in \"grab values from spenderq files\"\n",
    "    #ibatch values in groups of 50, so if ibatchrange is 100, the range is (50,100)\n",
    "    # for ibatch in range(0,2):\n",
    "    if len(ibatchrange) > 1:\n",
    "        start = ibatchrange[0]\n",
    "        end = ibatchrange[1]\n",
    "    else:\n",
    "        start = ibatchrange-50\n",
    "        end = ibatchrange\n",
    "    \n",
    "    for ibatch in range(start, end):\n",
    "        normalised_flux, pipeline_weight, z, tid, normalisation, zerr = [], [], [], [], [], []\n",
    "\n",
    "        sq_reconstructions = []\n",
    "        print('ibatch', ibatch)\n",
    "\n",
    "        #load batch\n",
    "        with open(f'{path}/{file_prefix}_%i.pkl' % ibatch, 'rb') as f:\n",
    "            _normalised_flux, _pipeline_weight, _z, _tid, _normalisation, _zerr = pickle.load(f)\n",
    "        normalised_flux.append(np.array(_normalised_flux))\n",
    "        pipeline_weight.append(np.array(_pipeline_weight))\n",
    "        z.append(np.array(_z))\n",
    "        tid.append(np.array(_tid))\n",
    "        normalisation.append(np.array(_normalisation))\n",
    "        zerr.append(np.array(_zerr))\n",
    "        \n",
    "        #load SpenderQ reconstructions\n",
    "        _sq_reconstructions = np.load(f'{path}/{file_prefix}_%i.recons.npy' % ibatch)\n",
    "        sq_reconstructions.append(np.array(_sq_reconstructions))\n",
    "\n",
    "        normalised_flux=np.concatenate(normalised_flux,axis=0)\n",
    "        pipeline_weight=np.concatenate(pipeline_weight,axis=0)\n",
    "        z=np.concatenate(z)\n",
    "        tid=np.concatenate(tid)\n",
    "        normalisation=np.concatenate(normalisation,axis=0)\n",
    "        zerr=np.concatenate(zerr,axis=0)\n",
    "        sq_reconstructions=np.concatenate(sq_reconstructions,axis=0)\n",
    "\n",
    "        #create variables needed for the delta files\n",
    "        #nb_of_quasars = 10\n",
    "        nb_of_quasars = len(z)\n",
    "        \n",
    "        _tff = np.full((nb_of_quasars,7781),np.nan)\n",
    "        _weight = np.full((nb_of_quasars,7781),np.nan)\n",
    "        _sq_cont = np.full((nb_of_quasars,7781),np.nan)\n",
    "        \n",
    "        wrecon = np.load(f'{path}/{file_prefix}.wave_recon.npy')\n",
    "        desi_wave = np.linspace(3600, 9824, 7781) #obs\n",
    "        \n",
    "        _tff_bar_up = np.zeros(7781)\n",
    "        _tff_bar_down = np.zeros(7781)\n",
    "        \n",
    "        #for iqso in range(10):\n",
    "        for iqso in range(nb_of_quasars):\n",
    "        \n",
    "            #create wavelength grids\n",
    "            desi_wave_rest = desi_wave/(1+z[iqso]) #rest\n",
    "            \n",
    "            #rebin spender reconstruction\n",
    "            edges = np.linspace(wrecon[0], wrecon[-1], 7782)\n",
    "            spenderq_rebin = U.trapz_rebin(wrecon, sq_reconstructions[iqso], edges = edges)\n",
    "            \n",
    "            #keep only part of spec that is within the lya range\n",
    "            mask_desi_rest = (desi_wave_rest >= forest_lambda_min) & (desi_wave_rest <= forest_lambda_max)\n",
    "        \n",
    "            _tff[iqso][mask_desi_rest] = normalised_flux[iqso][mask_desi_rest]/spenderq_rebin[mask_desi_rest]\n",
    "            _weight[iqso][mask_desi_rest] = pipeline_weight[iqso][mask_desi_rest]\n",
    "            _sq_cont[iqso][mask_desi_rest] = spenderq_rebin[mask_desi_rest]\n",
    "            _tff_bar_up += normalised_flux[iqso]/spenderq_rebin * pipeline_weight[iqso]\n",
    "            _tff_bar_down += pipeline_weight[iqso]\n",
    "\n",
    "        #get the weighted average transmitted flux fraction \n",
    "        picca_range = (desi_wave <= LAMBDA[-1])\n",
    "        tff_bar = np.divide(_tff_bar_up[picca_range], _tff_bar_down[picca_range], out=np.zeros_like(_tff_bar_up[picca_range]), where=_tff_bar_down[picca_range]!=0)\n",
    "\n",
    "        #picca structure: nan filled arrays\n",
    "        delta = np.full((nb_of_quasars,sum(picca_range)),np.nan)\n",
    "        weight = np.full((nb_of_quasars,sum(picca_range)),np.nan)\n",
    "        sq_cont = np.full((nb_of_quasars,sum(picca_range)),np.nan)\n",
    "        \n",
    "        meta_los_id, meta_ra, meta_dec, meta_z, meta_meansnr, meta_targetid, meta_night, meta_petal, meta_tile = [], [], [], [], [], [], [], [], []\n",
    "        \n",
    "        #for iqso in range(5):\n",
    "        for iqso in range(nb_of_quasars):\n",
    "            \n",
    "            delta[iqso] = (_tff[iqso][picca_range]/tff_bar) - 1.\n",
    "            weight[iqso] = _weight[iqso][picca_range]\n",
    "            sq_cont[iqso] = _sq_cont[iqso][picca_range]\n",
    "            \n",
    "            los_id_idx = int(np.where(catalogue_TID == int(tid[iqso]))[0][0])\n",
    "            \n",
    "            meta_los_id.append(int(catalogue_TID[los_id_idx]))\n",
    "            meta_ra.append(float(math.radians(catalogue_RA[los_id_idx])))\n",
    "            meta_dec.append(float(math.radians(catalogue_DEC[los_id_idx])))\n",
    "            meta_z.append(float(catalogue_Z[los_id_idx]))\n",
    "            meta_meansnr.append(np.nan)\n",
    "            meta_targetid.append(int(catalogue_TID[los_id_idx]))\n",
    "            meta_night.append('')\n",
    "            meta_petal.append('')\n",
    "            meta_tile.append('')\n",
    "\n",
    "        # Create primary HDU \n",
    "        primary_hdu = fits.PrimaryHDU(data=None)\n",
    "        \n",
    "        # Create OBSERVED WAVELENGTH HDU\n",
    "        hdu_wave = fits.ImageHDU(LAMBDA, name=f'LAMBDA')\n",
    "        hdu_wave.header['HIERARCH WAVE_SOLUTION'] = 'lin'\n",
    "        hdu_wave.header['HIERARCH DELTA_LAMBDA'] = 0.8\n",
    "        \n",
    "        # Set chunk size, and how to chunk\n",
    "        chunk_size = 1024\n",
    "        \n",
    "        \n",
    "        \n",
    "        _delta_chunks = chunks(delta, chunk_size)\n",
    "        _weight_chunks = chunks(weight, chunk_size)\n",
    "        _cont_chunks = chunks(sq_cont, chunk_size)\n",
    "        \n",
    "        nb_of_chunks = len(_delta_chunks)\n",
    "        print('num chunks', nb_of_chunks)\n",
    "        \n",
    "        for ichunk in range(nb_of_chunks):\n",
    "            i=0\n",
    "            c1 = fits.Column(name='LOS_ID', format='K', array=np.array(meta_los_id[i*chunk_size:(i+1)*chunk_size]))\n",
    "            c2 = fits.Column(name='RA', format='D', array=np.array(meta_ra[i*chunk_size:(i+1)*chunk_size]))\n",
    "            c3 = fits.Column(name='DEC', format='D', array=np.array(meta_dec[i*chunk_size:(i+1)*chunk_size]))\n",
    "            c4 = fits.Column(name='Z', format='D', array=np.array(meta_z[i*chunk_size:(i+1)*chunk_size]))\n",
    "            c5 = fits.Column(name='MEANSNR', format='D', array=meta_meansnr[i*chunk_size:(i+1)*chunk_size])\n",
    "            c6 = fits.Column(name='TARGETID', format='K', array=np.array(meta_targetid[i*chunk_size:(i+1)*chunk_size]))\n",
    "            c7 = fits.Column(name='NIGHT', format='12A', array=meta_night[i*chunk_size:(i+1)*chunk_size])\n",
    "            c8 = fits.Column(name='PETAL', format='12A', array=meta_petal[i*chunk_size:(i+1)*chunk_size])\n",
    "            c9 = fits.Column(name='TILE', format='12A', array=meta_tile[i*chunk_size:(i+1)*chunk_size])\n",
    "            hdu_meta = fits.BinTableHDU.from_columns([c1, c2, c3, c4, c5, c6, c7, c8, c9], name='METADATA')\n",
    "            hdu_meta.header['BLINDING'] = 'none'\n",
    "        \n",
    "            hdu_delta = fits.ImageHDU(_delta_chunks[ichunk], name=f'DELTA')\n",
    "            hdu_weight = fits.ImageHDU(_weight_chunks[ichunk], name=f'WEIGHT')\n",
    "            hdu_cont = fits.ImageHDU(_cont_chunks[ichunk], name=f'CONT')\n",
    "            hdu_tff_bar = fits.ImageHDU(tff_bar, name=f'FBAR')\n",
    "        \n",
    "            # Combine all HDUs into an HDUList\n",
    "            hdul = fits.HDUList([primary_hdu, hdu_wave, hdu_meta, hdu_delta, hdu_weight, hdu_cont, hdu_tff_bar])\n",
    "        \n",
    "            # Write the HDUList to a new FITS file\n",
    "            hdul.writeto(f'{outpath}/delta-%i.fits' % ibatch, overwrite=True)\n",
    "            i+=1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e13d0e2a-daef-4e65-be91-2c83a7729b66",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ibatch 0\n",
      "num chunks 1\n",
      "ibatch 1\n",
      "num chunks 1\n",
      "ibatch 2\n",
      "num chunks 1\n",
      "ibatch 3\n",
      "num chunks 1\n",
      "ibatch 4\n",
      "num chunks 1\n",
      "ibatch 5\n",
      "num chunks 1\n",
      "ibatch 6\n",
      "num chunks 1\n",
      "ibatch 7\n",
      "num chunks 1\n",
      "ibatch 8\n",
      "num chunks 1\n",
      "ibatch 9\n",
      "num chunks 1\n",
      "ibatch 10\n",
      "num chunks 1\n",
      "ibatch 11\n",
      "num chunks 1\n",
      "ibatch 12\n",
      "num chunks 1\n",
      "ibatch 13\n",
      "num chunks 1\n",
      "ibatch 14\n",
      "num chunks 1\n",
      "ibatch 15\n",
      "num chunks 1\n",
      "ibatch 16\n",
      "num chunks 1\n",
      "ibatch 17\n",
      "num chunks 1\n",
      "ibatch 18\n",
      "num chunks 1\n",
      "ibatch 19\n",
      "num chunks 1\n",
      "ibatch 20\n",
      "num chunks 1\n",
      "ibatch 21\n",
      "num chunks 1\n",
      "ibatch 22\n",
      "num chunks 1\n",
      "ibatch 23\n",
      "num chunks 1\n",
      "ibatch 24\n",
      "num chunks 1\n",
      "ibatch 25\n",
      "num chunks 1\n",
      "ibatch 26\n",
      "num chunks 1\n",
      "ibatch 27\n",
      "num chunks 1\n",
      "ibatch 28\n",
      "num chunks 1\n",
      "ibatch 29\n",
      "num chunks 1\n",
      "ibatch 30\n",
      "num chunks 1\n",
      "ibatch 31\n",
      "num chunks 1\n",
      "ibatch 32\n",
      "num chunks 1\n",
      "ibatch 33\n",
      "num chunks 1\n",
      "ibatch 34\n",
      "num chunks 1\n",
      "ibatch 35\n",
      "num chunks 1\n",
      "ibatch 36\n",
      "num chunks 1\n",
      "ibatch 37\n",
      "num chunks 1\n",
      "ibatch 38\n",
      "num chunks 1\n",
      "ibatch 39\n",
      "num chunks 1\n",
      "ibatch 40\n",
      "num chunks 1\n",
      "ibatch 41\n",
      "num chunks 1\n",
      "ibatch 42\n",
      "num chunks 1\n",
      "ibatch 43\n",
      "num chunks 1\n",
      "ibatch 44\n",
      "num chunks 1\n",
      "ibatch 45\n",
      "num chunks 1\n",
      "ibatch 46\n",
      "num chunks 1\n",
      "ibatch 47\n",
      "num chunks 1\n",
      "ibatch 48\n",
      "num chunks 1\n",
      "ibatch 49\n",
      "num chunks 1\n"
     ]
    }
   ],
   "source": [
    "spender_to_deltas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d0f0ea9-7d88-43c3-8fa7-43de9c35e4c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ibatch 50\n",
      "num chunks 1\n",
      "ibatch 51\n",
      "num chunks 1\n",
      "ibatch 52\n",
      "num chunks 1\n",
      "ibatch 53\n",
      "num chunks 1\n",
      "ibatch 54\n",
      "num chunks 1\n",
      "ibatch 55\n",
      "num chunks 1\n",
      "ibatch 56\n",
      "num chunks 1\n",
      "ibatch 57\n",
      "num chunks 1\n",
      "ibatch 58\n",
      "num chunks 1\n",
      "ibatch 59\n",
      "num chunks 1\n",
      "ibatch 60\n",
      "num chunks 1\n",
      "ibatch 61\n",
      "num chunks 1\n",
      "ibatch 62\n",
      "num chunks 1\n",
      "ibatch 63\n",
      "num chunks 1\n",
      "ibatch 64\n",
      "num chunks 1\n",
      "ibatch 65\n",
      "num chunks 1\n",
      "ibatch 66\n",
      "num chunks 1\n",
      "ibatch 67\n",
      "num chunks 1\n",
      "ibatch 68\n",
      "num chunks 1\n",
      "ibatch 69\n",
      "num chunks 1\n",
      "ibatch 70\n",
      "num chunks 1\n",
      "ibatch 71\n",
      "num chunks 1\n",
      "ibatch 72\n",
      "num chunks 1\n",
      "ibatch 73\n",
      "num chunks 1\n",
      "ibatch 74\n",
      "num chunks 1\n",
      "ibatch 75\n",
      "num chunks 1\n",
      "ibatch 76\n",
      "num chunks 1\n",
      "ibatch 77\n",
      "num chunks 1\n",
      "ibatch 78\n",
      "num chunks 1\n",
      "ibatch 79\n",
      "num chunks 1\n",
      "ibatch 80\n",
      "num chunks 1\n",
      "ibatch 81\n",
      "num chunks 1\n",
      "ibatch 82\n",
      "num chunks 1\n",
      "ibatch 83\n",
      "num chunks 1\n",
      "ibatch 84\n",
      "num chunks 1\n",
      "ibatch 85\n",
      "num chunks 1\n",
      "ibatch 86\n",
      "num chunks 1\n",
      "ibatch 87\n",
      "num chunks 1\n",
      "ibatch 88\n",
      "num chunks 1\n",
      "ibatch 89\n",
      "num chunks 1\n",
      "ibatch 90\n",
      "num chunks 1\n",
      "ibatch 91\n",
      "num chunks 1\n",
      "ibatch 92\n",
      "num chunks 1\n",
      "ibatch 93\n",
      "num chunks 1\n",
      "ibatch 94\n",
      "num chunks 1\n",
      "ibatch 95\n",
      "num chunks 1\n",
      "ibatch 96\n",
      "num chunks 1\n",
      "ibatch 97\n",
      "num chunks 1\n",
      "ibatch 98\n",
      "num chunks 1\n",
      "ibatch 99\n",
      "num chunks 1\n"
     ]
    }
   ],
   "source": [
    "spender_to_deltas(ibatchrange = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad07062c-49ed-4c0c-8c3e-7c23d2527bde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ibatch 100\n",
      "num chunks 1\n",
      "ibatch 101\n",
      "num chunks 1\n",
      "ibatch 102\n",
      "num chunks 1\n",
      "ibatch 103\n",
      "num chunks 1\n",
      "ibatch 104\n",
      "num chunks 1\n",
      "ibatch 105\n",
      "num chunks 1\n",
      "ibatch 106\n",
      "num chunks 1\n",
      "ibatch 107\n",
      "num chunks 1\n",
      "ibatch 108\n",
      "num chunks 1\n",
      "ibatch 109\n",
      "num chunks 1\n",
      "ibatch 110\n",
      "num chunks 1\n",
      "ibatch 111\n",
      "num chunks 1\n",
      "ibatch 112\n",
      "num chunks 1\n",
      "ibatch 113\n",
      "num chunks 1\n",
      "ibatch 114\n",
      "num chunks 1\n",
      "ibatch 115\n",
      "num chunks 1\n",
      "ibatch 116\n",
      "num chunks 1\n",
      "ibatch 117\n",
      "num chunks 1\n",
      "ibatch 118\n",
      "num chunks 1\n",
      "ibatch 119\n",
      "num chunks 1\n",
      "ibatch 120\n",
      "num chunks 1\n",
      "ibatch 121\n",
      "num chunks 1\n",
      "ibatch 122\n",
      "num chunks 1\n",
      "ibatch 123\n",
      "num chunks 1\n",
      "ibatch 124\n",
      "num chunks 1\n",
      "ibatch 125\n",
      "num chunks 1\n",
      "ibatch 126\n",
      "num chunks 1\n",
      "ibatch 127\n",
      "num chunks 1\n",
      "ibatch 128\n",
      "num chunks 1\n",
      "ibatch 129\n",
      "num chunks 1\n",
      "ibatch 130\n",
      "num chunks 1\n",
      "ibatch 131\n",
      "num chunks 1\n",
      "ibatch 132\n",
      "num chunks 1\n",
      "ibatch 133\n",
      "num chunks 1\n",
      "ibatch 134\n",
      "num chunks 1\n",
      "ibatch 135\n",
      "num chunks 1\n",
      "ibatch 136\n",
      "num chunks 1\n",
      "ibatch 137\n",
      "num chunks 1\n",
      "ibatch 138\n",
      "num chunks 1\n",
      "ibatch 139\n",
      "num chunks 1\n",
      "ibatch 140\n",
      "num chunks 1\n",
      "ibatch 141\n",
      "num chunks 1\n",
      "ibatch 142\n",
      "num chunks 1\n",
      "ibatch 143\n",
      "num chunks 1\n",
      "ibatch 144\n",
      "num chunks 1\n",
      "ibatch 145\n",
      "num chunks 1\n",
      "ibatch 146\n",
      "num chunks 1\n",
      "ibatch 147\n",
      "num chunks 1\n",
      "ibatch 148\n",
      "num chunks 1\n",
      "ibatch 149\n",
      "num chunks 1\n",
      "ibatch 150\n",
      "num chunks 1\n",
      "ibatch 151\n",
      "num chunks 1\n",
      "ibatch 152\n",
      "num chunks 1\n",
      "ibatch 153\n",
      "num chunks 1\n",
      "ibatch 154\n",
      "num chunks 1\n",
      "ibatch 155\n",
      "num chunks 1\n",
      "ibatch 156\n",
      "num chunks 1\n",
      "ibatch 157\n",
      "num chunks 1\n",
      "ibatch 158\n",
      "num chunks 1\n",
      "ibatch 159\n",
      "num chunks 1\n",
      "ibatch 160\n",
      "num chunks 1\n",
      "ibatch 161\n",
      "num chunks 1\n",
      "ibatch 162\n",
      "num chunks 1\n",
      "ibatch 163\n",
      "num chunks 1\n",
      "ibatch 164\n",
      "num chunks 1\n",
      "ibatch 165\n",
      "num chunks 1\n",
      "ibatch 166\n",
      "num chunks 1\n",
      "ibatch 167\n",
      "num chunks 1\n",
      "ibatch 168\n",
      "num chunks 1\n",
      "ibatch 169\n",
      "num chunks 1\n",
      "ibatch 170\n",
      "num chunks 1\n",
      "ibatch 171\n",
      "num chunks 1\n",
      "ibatch 172\n",
      "num chunks 1\n",
      "ibatch 173\n",
      "num chunks 1\n",
      "ibatch 174\n",
      "num chunks 1\n",
      "ibatch 175\n",
      "num chunks 1\n",
      "ibatch 176\n",
      "num chunks 1\n",
      "ibatch 177\n",
      "num chunks 1\n",
      "ibatch 178\n",
      "num chunks 1\n",
      "ibatch 179\n",
      "num chunks 1\n",
      "ibatch 180\n",
      "num chunks 1\n",
      "ibatch 181\n",
      "num chunks 1\n",
      "ibatch 182\n",
      "num chunks 1\n",
      "ibatch 183\n",
      "num chunks 1\n",
      "ibatch 184\n",
      "num chunks 1\n",
      "ibatch 185\n",
      "num chunks 1\n",
      "ibatch 186\n",
      "num chunks 1\n",
      "ibatch 187\n",
      "num chunks 1\n",
      "ibatch 188\n",
      "num chunks 1\n",
      "ibatch 189\n",
      "num chunks 1\n",
      "ibatch 190\n",
      "num chunks 1\n",
      "ibatch 191\n",
      "num chunks 1\n",
      "ibatch 192\n",
      "num chunks 1\n",
      "ibatch 193\n",
      "num chunks 1\n",
      "ibatch 194\n",
      "num chunks 1\n",
      "ibatch 195\n",
      "num chunks 1\n",
      "ibatch 196\n",
      "num chunks 1\n",
      "ibatch 197\n",
      "num chunks 1\n",
      "ibatch 198\n",
      "num chunks 1\n",
      "ibatch 199\n",
      "num chunks 1\n",
      "ibatch 200\n",
      "num chunks 1\n",
      "ibatch 201\n",
      "num chunks 1\n",
      "ibatch 202\n",
      "num chunks 1\n",
      "ibatch 203\n",
      "num chunks 1\n",
      "ibatch 204\n",
      "num chunks 1\n",
      "ibatch 205\n",
      "num chunks 1\n",
      "ibatch 206\n",
      "num chunks 1\n",
      "ibatch 207\n",
      "num chunks 1\n",
      "ibatch 208\n",
      "num chunks 1\n",
      "ibatch 209\n",
      "num chunks 1\n",
      "ibatch 210\n",
      "num chunks 1\n",
      "ibatch 211\n",
      "num chunks 1\n",
      "ibatch 212\n",
      "num chunks 1\n",
      "ibatch 213\n",
      "num chunks 1\n",
      "ibatch 214\n",
      "num chunks 1\n",
      "ibatch 215\n",
      "num chunks 1\n",
      "ibatch 216\n",
      "num chunks 1\n",
      "ibatch 217\n",
      "num chunks 1\n",
      "ibatch 218\n",
      "num chunks 1\n",
      "ibatch 219\n",
      "num chunks 1\n",
      "ibatch 220\n",
      "num chunks 1\n",
      "ibatch 221\n",
      "num chunks 1\n",
      "ibatch 222\n",
      "num chunks 1\n",
      "ibatch 223\n",
      "num chunks 1\n",
      "ibatch 224\n",
      "num chunks 1\n",
      "ibatch 225\n",
      "num chunks 1\n",
      "ibatch 226\n",
      "num chunks 1\n",
      "ibatch 227\n",
      "num chunks 1\n",
      "ibatch 228\n",
      "num chunks 1\n",
      "ibatch 229\n",
      "num chunks 1\n",
      "ibatch 230\n",
      "num chunks 1\n",
      "ibatch 231\n",
      "num chunks 1\n",
      "ibatch 232\n",
      "num chunks 1\n",
      "ibatch 233\n",
      "num chunks 1\n",
      "ibatch 234\n",
      "num chunks 1\n",
      "ibatch 235\n",
      "num chunks 1\n",
      "ibatch 236\n",
      "num chunks 1\n",
      "ibatch 237\n",
      "num chunks 1\n",
      "ibatch 238\n",
      "num chunks 1\n",
      "ibatch 239\n",
      "num chunks 1\n",
      "ibatch 240\n",
      "num chunks 1\n",
      "ibatch 241\n",
      "num chunks 1\n",
      "ibatch 242\n",
      "num chunks 1\n",
      "ibatch 243\n",
      "num chunks 1\n",
      "ibatch 244\n",
      "num chunks 1\n",
      "ibatch 245\n",
      "num chunks 1\n",
      "ibatch 246\n",
      "num chunks 1\n",
      "ibatch 247\n",
      "num chunks 1\n",
      "ibatch 248\n",
      "num chunks 1\n",
      "ibatch 249\n",
      "num chunks 1\n",
      "ibatch 250\n",
      "num chunks 1\n",
      "ibatch 251\n",
      "num chunks 1\n",
      "ibatch 252\n",
      "num chunks 1\n",
      "ibatch 253\n",
      "num chunks 1\n",
      "ibatch 254\n",
      "num chunks 1\n",
      "ibatch 255\n",
      "num chunks 1\n",
      "ibatch 256\n",
      "num chunks 1\n",
      "ibatch 257\n",
      "num chunks 1\n",
      "ibatch 258\n",
      "num chunks 1\n",
      "ibatch 259\n",
      "num chunks 1\n",
      "ibatch 260\n",
      "num chunks 1\n",
      "ibatch 261\n",
      "num chunks 1\n",
      "ibatch 262\n",
      "num chunks 1\n",
      "ibatch 263\n",
      "num chunks 1\n",
      "ibatch 264\n",
      "num chunks 1\n",
      "ibatch 265\n",
      "num chunks 1\n",
      "ibatch 266\n",
      "num chunks 1\n",
      "ibatch 267\n",
      "num chunks 1\n",
      "ibatch 268\n",
      "num chunks 1\n",
      "ibatch 269\n",
      "num chunks 1\n",
      "ibatch 270\n",
      "num chunks 1\n",
      "ibatch 271\n",
      "num chunks 1\n",
      "ibatch 272\n",
      "num chunks 1\n",
      "ibatch 273\n",
      "num chunks 1\n",
      "ibatch 274\n",
      "num chunks 1\n",
      "ibatch 275\n",
      "num chunks 1\n",
      "ibatch 276\n",
      "num chunks 1\n",
      "ibatch 277\n",
      "num chunks 1\n",
      "ibatch 278\n",
      "num chunks 1\n",
      "ibatch 279\n",
      "num chunks 1\n",
      "ibatch 280\n",
      "num chunks 1\n",
      "ibatch 281\n",
      "num chunks 1\n",
      "ibatch 282\n",
      "num chunks 1\n",
      "ibatch 283\n",
      "num chunks 1\n",
      "ibatch 284\n",
      "num chunks 1\n",
      "ibatch 285\n",
      "num chunks 1\n",
      "ibatch 286\n",
      "num chunks 1\n",
      "ibatch 287\n",
      "num chunks 1\n",
      "ibatch 288\n",
      "num chunks 1\n",
      "ibatch 289\n",
      "num chunks 1\n",
      "ibatch 290\n",
      "num chunks 1\n",
      "ibatch 291\n",
      "num chunks 1\n",
      "ibatch 292\n",
      "num chunks 1\n",
      "ibatch 293\n",
      "num chunks 1\n",
      "ibatch 294\n",
      "num chunks 1\n",
      "ibatch 295\n",
      "num chunks 1\n",
      "ibatch 296\n",
      "num chunks 1\n",
      "ibatch 297\n",
      "num chunks 1\n",
      "ibatch 298\n",
      "num chunks 1\n",
      "ibatch 299\n",
      "num chunks 1\n",
      "ibatch 300\n",
      "num chunks 1\n",
      "ibatch 301\n",
      "num chunks 1\n",
      "ibatch 302\n",
      "num chunks 1\n",
      "ibatch 303\n",
      "num chunks 1\n",
      "ibatch 304\n",
      "num chunks 1\n",
      "ibatch 305\n",
      "num chunks 1\n",
      "ibatch 306\n",
      "num chunks 1\n",
      "ibatch 307\n",
      "num chunks 1\n",
      "ibatch 308\n",
      "num chunks 1\n",
      "ibatch 309\n",
      "num chunks 1\n",
      "ibatch 310\n",
      "num chunks 1\n",
      "ibatch 311\n",
      "num chunks 1\n",
      "ibatch 312\n",
      "num chunks 1\n",
      "ibatch 313\n",
      "num chunks 1\n",
      "ibatch 314\n",
      "num chunks 1\n",
      "ibatch 315\n",
      "num chunks 1\n",
      "ibatch 316\n",
      "num chunks 1\n",
      "ibatch 317\n",
      "num chunks 1\n",
      "ibatch 318\n",
      "num chunks 1\n",
      "ibatch 319\n",
      "num chunks 1\n",
      "ibatch 320\n",
      "num chunks 1\n",
      "ibatch 321\n",
      "num chunks 1\n",
      "ibatch 322\n",
      "num chunks 1\n",
      "ibatch 323\n",
      "num chunks 1\n",
      "ibatch 324\n",
      "num chunks 1\n",
      "ibatch 325\n",
      "num chunks 1\n",
      "ibatch 326\n",
      "num chunks 1\n",
      "ibatch 327\n",
      "num chunks 1\n",
      "ibatch 328\n",
      "num chunks 1\n",
      "ibatch 329\n",
      "num chunks 1\n",
      "ibatch 330\n",
      "num chunks 1\n",
      "ibatch 331\n",
      "num chunks 1\n",
      "ibatch 332\n",
      "num chunks 1\n",
      "ibatch 333\n",
      "num chunks 1\n",
      "ibatch 334\n",
      "num chunks 1\n",
      "ibatch 335\n",
      "num chunks 1\n",
      "ibatch 336\n",
      "num chunks 1\n",
      "ibatch 337\n",
      "num chunks 1\n",
      "ibatch 338\n",
      "num chunks 1\n",
      "ibatch 339\n",
      "num chunks 1\n",
      "ibatch 340\n",
      "num chunks 1\n",
      "ibatch 341\n",
      "num chunks 1\n",
      "ibatch 342\n",
      "num chunks 1\n",
      "ibatch 343\n",
      "num chunks 1\n",
      "ibatch 344\n",
      "num chunks 1\n",
      "ibatch 345\n",
      "num chunks 1\n",
      "ibatch 346\n",
      "num chunks 1\n",
      "ibatch 347\n",
      "num chunks 1\n",
      "ibatch 348\n",
      "num chunks 1\n",
      "ibatch 349\n",
      "num chunks 1\n",
      "ibatch 350\n",
      "num chunks 1\n",
      "ibatch 351\n",
      "num chunks 1\n",
      "ibatch 352\n",
      "num chunks 1\n",
      "ibatch 353\n",
      "num chunks 1\n",
      "ibatch 354\n",
      "num chunks 1\n",
      "ibatch 355\n",
      "num chunks 1\n",
      "ibatch 356\n",
      "num chunks 1\n",
      "ibatch 357\n",
      "num chunks 1\n",
      "ibatch 358\n",
      "num chunks 1\n",
      "ibatch 359\n",
      "num chunks 1\n",
      "ibatch 360\n",
      "num chunks 1\n",
      "ibatch 361\n",
      "num chunks 1\n",
      "ibatch 362\n",
      "num chunks 1\n",
      "ibatch 363\n",
      "num chunks 1\n",
      "ibatch 364\n",
      "num chunks 1\n",
      "ibatch 365\n",
      "num chunks 1\n",
      "ibatch 366\n",
      "num chunks 1\n",
      "ibatch 367\n",
      "num chunks 1\n",
      "ibatch 368\n",
      "num chunks 1\n",
      "ibatch 369\n",
      "num chunks 1\n",
      "ibatch 370\n",
      "num chunks 1\n",
      "ibatch 371\n",
      "num chunks 1\n",
      "ibatch 372\n",
      "num chunks 1\n",
      "ibatch 373\n",
      "num chunks 1\n",
      "ibatch 374\n",
      "num chunks 1\n",
      "ibatch 375\n",
      "num chunks 1\n",
      "ibatch 376\n",
      "num chunks 1\n",
      "ibatch 377\n",
      "num chunks 1\n",
      "ibatch 378\n",
      "num chunks 1\n",
      "ibatch 379\n",
      "num chunks 1\n",
      "ibatch 380\n",
      "num chunks 1\n",
      "ibatch 381\n",
      "num chunks 1\n",
      "ibatch 382\n",
      "num chunks 1\n",
      "ibatch 383\n",
      "num chunks 1\n",
      "ibatch 384\n",
      "num chunks 1\n",
      "ibatch 385\n",
      "num chunks 1\n",
      "ibatch 386\n",
      "num chunks 1\n",
      "ibatch 387\n",
      "num chunks 1\n",
      "ibatch 388\n",
      "num chunks 1\n",
      "ibatch 389\n",
      "num chunks 1\n",
      "ibatch 390\n",
      "num chunks 1\n",
      "ibatch 391\n",
      "num chunks 1\n",
      "ibatch 392\n",
      "num chunks 1\n"
     ]
    }
   ],
   "source": [
    "spender_to_deltas(ibatchrange = (100, 393))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8dee0d-e79e-44be-8bf6-908903d88e8d",
   "metadata": {},
   "source": [
    "# old code that was copied into the above function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f3f5a-4337-4c1c-86d8-d599f1a3605d",
   "metadata": {},
   "source": [
    "## Grab values from SpenderQ files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12a6dcee-ed5a-4719-a0d0-c15c5e7027ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "normalised_flux, pipeline_weight, z, tid, normalisation, zerr = [], [], [], [], [], []\n",
    "\n",
    "sq_reconstructions = []\n",
    "\n",
    "for ibatch in range(0,1):\n",
    "    print(ibatch)\n",
    "#for ibatch in range(len(spender_output_files)): \n",
    "    \n",
    "    #load batch\n",
    "    with open(f'{path}/{file_prefix}_%i.pkl' % ibatch, 'rb') as f:\n",
    "        _normalised_flux, _pipeline_weight, _z, _tid, _normalisation, _zerr = pickle.load(f)\n",
    "    normalised_flux.append(np.array(_normalised_flux))\n",
    "    pipeline_weight.append(np.array(_pipeline_weight))\n",
    "    z.append(np.array(_z))\n",
    "    tid.append(np.array(_tid))\n",
    "    normalisation.append(np.array(_normalisation))\n",
    "    zerr.append(np.array(_zerr))\n",
    "    \n",
    "    #load SpenderQ reconstructions\n",
    "    _sq_reconstructions = np.load(f'{path}/{file_prefix}_%i.recons.npy' % ibatch)\n",
    "    sq_reconstructions.append(np.array(_sq_reconstructions))\n",
    "\n",
    "normalised_flux=np.concatenate(normalised_flux,axis=0)\n",
    "pipeline_weight=np.concatenate(pipeline_weight,axis=0)\n",
    "z=np.concatenate(z)\n",
    "tid=np.concatenate(tid)\n",
    "normalisation=np.concatenate(normalisation,axis=0)\n",
    "zerr=np.concatenate(zerr,axis=0)\n",
    "sq_reconstructions=np.concatenate(sq_reconstructions,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb25bef-9125-4d66-81f0-2e828e163153",
   "metadata": {},
   "source": [
    "## Create variables needed for delta files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9f52612-4cbd-4a36-b259-cc1dd37ad254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nb_of_quasars = 10\n",
    "nb_of_quasars = len(z)\n",
    "\n",
    "_tff = np.full((nb_of_quasars,7781),np.nan)\n",
    "_weight = np.full((nb_of_quasars,7781),np.nan)\n",
    "_sq_cont = np.full((nb_of_quasars,7781),np.nan)\n",
    "\n",
    "wrecon = np.load(f'{path}/{file_prefix}.wave_recon.npy')\n",
    "desi_wave = np.linspace(3600, 9824, 7781) #obs\n",
    "\n",
    "_tff_bar_up = np.zeros(7781)\n",
    "_tff_bar_down = np.zeros(7781)\n",
    "\n",
    "#for iqso in range(10):\n",
    "for iqso in range(nb_of_quasars):\n",
    "\n",
    "    #create wavelength grids\n",
    "    desi_wave_rest = desi_wave/(1+z[iqso]) #rest\n",
    "    \n",
    "    #rebin spender reconstruction\n",
    "    edges = np.linspace(wrecon[0], wrecon[-1], 7782)\n",
    "    spenderq_rebin = U.trapz_rebin(wrecon, sq_reconstructions[iqso], edges = edges)\n",
    "    \n",
    "    #keep only part of spec that is within the lya range\n",
    "    mask_desi_rest = (desi_wave_rest >= forest_lambda_min) & (desi_wave_rest <= forest_lambda_max)\n",
    "\n",
    "    _tff[iqso][mask_desi_rest] = normalised_flux[iqso][mask_desi_rest]/spenderq_rebin[mask_desi_rest]\n",
    "    _weight[iqso][mask_desi_rest] = pipeline_weight[iqso][mask_desi_rest]\n",
    "    _sq_cont[iqso][mask_desi_rest] = spenderq_rebin[mask_desi_rest]\n",
    "    _tff_bar_up += normalised_flux[iqso]/spenderq_rebin * pipeline_weight[iqso]\n",
    "    _tff_bar_down += pipeline_weight[iqso]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "525691a3-3409-4eef-a140-3103b95ad18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the weighted average transmitted flux fraction \n",
    "picca_range = (desi_wave <= LAMBDA[-1])\n",
    "tff_bar = np.divide(_tff_bar_up[picca_range], _tff_bar_down[picca_range], out=np.zeros_like(_tff_bar_up[picca_range]), where=_tff_bar_down[picca_range]!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d58cac43-616a-480e-b6b0-3b12d330f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#picca structure: nan filled arrays\n",
    "delta = np.full((nb_of_quasars,sum(picca_range)),np.nan)\n",
    "weight = np.full((nb_of_quasars,sum(picca_range)),np.nan)\n",
    "sq_cont = np.full((nb_of_quasars,sum(picca_range)),np.nan)\n",
    "\n",
    "meta_los_id, meta_ra, meta_dec, meta_z, meta_meansnr, meta_targetid, meta_night, meta_petal, meta_tile = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "#for iqso in range(5):\n",
    "for iqso in range(nb_of_quasars):\n",
    "    \n",
    "    delta[iqso] = (_tff[iqso][picca_range]/tff_bar) - 1.\n",
    "    weight[iqso] = _weight[iqso][picca_range]\n",
    "    sq_cont[iqso] = _sq_cont[iqso][picca_range]\n",
    "    \n",
    "    los_id_idx = int(np.where(catalogue_TID == int(tid[iqso]))[0][0])\n",
    "    \n",
    "    meta_los_id.append(int(catalogue_TID[los_id_idx]))\n",
    "    meta_ra.append(float(math.radians(catalogue_RA[los_id_idx])))\n",
    "    meta_dec.append(float(math.radians(catalogue_DEC[los_id_idx])))\n",
    "    meta_z.append(float(catalogue_Z[los_id_idx]))\n",
    "    meta_meansnr.append(np.nan)\n",
    "    meta_targetid.append(int(catalogue_TID[los_id_idx]))\n",
    "    meta_night.append('')\n",
    "    meta_petal.append('')\n",
    "    meta_tile.append('')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da7ce5-6737-4473-9f5f-459ffe0a24e7",
   "metadata": {},
   "source": [
    "## Create the delta files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a679268d-cf29-4697-9bfb-f5eabdedcdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Create primary HDU \n",
    "primary_hdu = fits.PrimaryHDU(data=None)\n",
    "\n",
    "# Create OBSERVED WAVELENGTH HDU\n",
    "hdu_wave = fits.ImageHDU(LAMBDA, name=f'LAMBDA')\n",
    "hdu_wave.header['HIERARCH WAVE_SOLUTION'] = 'lin'\n",
    "hdu_wave.header['HIERARCH DELTA_LAMBDA'] = 0.8\n",
    "\n",
    "# Set chunk size, and how to chunk\n",
    "chunk_size = 1024\n",
    "\n",
    "def chunks(lst, n):\n",
    "    return [lst[i:i + n] for i in range(0, len(lst), n)]\n",
    "\n",
    "_delta_chunks = chunks(delta, chunk_size)\n",
    "_weight_chunks = chunks(weight, chunk_size)\n",
    "_cont_chunks = chunks(sq_cont, chunk_size)\n",
    "\n",
    "nb_of_chunks = len(_delta_chunks)\n",
    "print(nb_of_chunks)\n",
    "\n",
    "for ichunk in range(nb_of_chunks):\n",
    "    i=0\n",
    "    c1 = fits.Column(name='LOS_ID', format='K', array=np.array(meta_los_id[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c2 = fits.Column(name='RA', format='D', array=np.array(meta_ra[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c3 = fits.Column(name='DEC', format='D', array=np.array(meta_dec[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c4 = fits.Column(name='Z', format='D', array=np.array(meta_z[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c5 = fits.Column(name='MEANSNR', format='D', array=meta_meansnr[i*chunk_size:(i+1)*chunk_size])\n",
    "    c6 = fits.Column(name='TARGETID', format='K', array=np.array(meta_targetid[i*chunk_size:(i+1)*chunk_size]))\n",
    "    c7 = fits.Column(name='NIGHT', format='12A', array=meta_night[i*chunk_size:(i+1)*chunk_size])\n",
    "    c8 = fits.Column(name='PETAL', format='12A', array=meta_petal[i*chunk_size:(i+1)*chunk_size])\n",
    "    c9 = fits.Column(name='TILE', format='12A', array=meta_tile[i*chunk_size:(i+1)*chunk_size])\n",
    "    hdu_meta = fits.BinTableHDU.from_columns([c1, c2, c3, c4, c5, c6, c7, c8, c9], name='METADATA')\n",
    "    hdu_meta.header['BLINDING'] = 'none'\n",
    "\n",
    "    hdu_delta = fits.ImageHDU(_delta_chunks[ichunk], name=f'DELTA')\n",
    "    hdu_weight = fits.ImageHDU(_weight_chunks[ichunk], name=f'WEIGHT')\n",
    "    hdu_cont = fits.ImageHDU(_cont_chunks[ichunk], name=f'CONT')\n",
    "    hdu_tff_bar = fits.ImageHDU(tff_bar, name=f'FBAR')\n",
    "\n",
    "    # Combine all HDUs into an HDUList\n",
    "    hdul = fits.HDUList([primary_hdu, hdu_wave, hdu_meta, hdu_delta, hdu_weight, hdu_cont, hdu_tff_bar])\n",
    "\n",
    "    # Write the HDUList to a new FITS file\n",
    "    hdul.writeto(f'{outpath}/delta-%i.fits' % ichunk, overwrite=True)\n",
    "    i+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8146aac7-2676-4d80-b7ec-f27af8a470e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spender",
   "language": "python",
   "name": "spender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
